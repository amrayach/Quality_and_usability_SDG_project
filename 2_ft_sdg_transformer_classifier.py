from flair.embeddings import TransformerDocumentEmbeddings
from flair.models import TextClassifier
from flair.trainers import ModelTrainer
from flair.data import Corpus
from flair.datasets import CSVClassificationCorpus
import pandas as pd
import numpy as np
from sklearn.utils import class_weight


# Please check the following link for more details about the class_weight.compute_class_weight function
# https://stackoverflow.com/questions/69783897/compute-class-weight-function-issue-in-sklearn-library-when-used-in-keras-cl

def create_weight_dict(data_path, train_with_dev, strict_weights):
    # depending on the value of train_with_dev, the function will either include the dev set for the weights computation or not
    if train_with_dev:
        train_dataset = pd.read_csv(data_path + 'train.csv', delimiter="\t", header=None, index_col=False,
                                    lineterminator='\n')
        dev_dataset = pd.read_csv(data_path + 'dev.csv', delimiter="\t", header=None, index_col=False,
                                  lineterminator='\n')
        comp_data = pd.concat([train_dataset, dev_dataset], ignore_index=True)
    else:
        comp_data = pd.read_csv(data_path + 'train.csv', delimiter="\t", header=None, index_col=False,
                                lineterminator='\n')

    # unique_labels contains the unique labels in the dataset
    unique_labels = np.unique(comp_data.iloc[:, 0])
    labels = comp_data.iloc[:, 0]

    # call the class_weight.compute_class_weight function to compute the weights

    # Balanced weights
    if strict_weights == 1:
        class_weights = class_weight.compute_class_weight('balanced',
                                                          classes=unique_labels,
                                                          y=labels)
    # Unbalanced weights
    elif strict_weights == 2:
        class_weights = class_weight.compute_class_weight(None,
                                                          classes=unique_labels,
                                                          y=labels)

    # fill the dictionary with the computed weights properly
    class_weights_dict = {}
    for unique_labels, class_weights in zip(unique_labels, class_weights):
        class_weights_dict[unique_labels] = class_weights

    # return the populated dictionary
    return class_weights_dict


def main():
    # Stratified 5-fold cross-validation generated data
    # this should be already generated by the previous script: 1_prep_dataset.py
    data_folder = "./Data_merged_all/"

    # This is where the models will be saved for each fold
    model_dir = "./Models_merged_all/"

    # indicates whether to use the dev set for training or not
    train_with_dev = True

    # Selects the transformer model to use
    transformer_name = "bert-base-cased"

    # Selects the weight computation method for the loss function
    # 1: balanced
    # 2: None
    # 3: custom (uniform) based on a fast analysis of the data
    # The best results were obtained with 1 (recommended)
    strict_weights = 1

    # iterate over the 5 folds
    for i in range(5):
        print("Fold: " + str(i + 1))
        print("Data-Folder: " + data_folder + str(i + 1) + "/")
        print("Model-Folder: " + model_dir + str(i + 1) + "/")

        # generate weight dictionary for the loss function
        # according to the selected method
        if strict_weights == 1:
            weight_dict = create_weight_dict(data_folder + str(i + 1) + "/", train_with_dev, strict_weights)
        elif strict_weights == 2:
            weight_dict = create_weight_dict(data_folder + str(i + 1) + "/", train_with_dev, strict_weights)
        else:
            weight_dict = {'__label__0': 0.058823529,
                           '__label__1': 0.058823529,
                           '__label__2': 0.058823529,
                           '__label__3': 0.058823529,
                           '__label__4': 0.058823529,
                           '__label__5': 0.058823529,
                           '__label__6': 0.058823529,
                           '__label__7': 0.058823529,
                           '__label__8': 0.058823529,
                           '__label__9': 0.058823529,
                           '__label__10': 0.058823529,
                           '__label__11': 0.058823529,
                           '__label__12': 0.058823529,
                           '__label__13': 0.058823529,
                           '__label__14': 0.058823529,
                           '__label__15': 0.058823529,
                           '__label__16': 0.058823529}

        # sort weight_dict by key using lambda function
        weight_dict = dict(sorted(weight_dict.items(), key=lambda item: int(item[0].split("__")[-1])))

        # initialize the CSV classification corpus
        # pass the path to the data folder (respectively to current fold)
        # label type: status --> sdg status
        # column_name_map: maps the column names to the correct names
        # delimiter: tab
        # train_file: train.csv
        # dev_file: dev.csv
        # test_file: test.csv
        corpus: Corpus = CSVClassificationCorpus(data_folder + str(i + 1) + "/", label_type="status",
                                                 column_name_map={0: "label_status", 1: "text"},
                                                 delimiter='\t', train_file="train.csv", dev_file="dev.csv",
                                                 test_file="test.csv")

        # create the label dictionary from the corpus
        # this is needed for the classifier
        # the label dictionary maps the labels to integers
        label_dict_csv = corpus.make_label_dictionary("status")

        # initialize the document embeddings, which is the core transformer model
        # transformer_name: bert-base-cased
        # fine_tune: True --> fine-tune the transformer model
        document_embeddings = TransformerDocumentEmbeddings(transformer_name, fine_tune=True)

        # initialize the text classifier, which is the classifier that uses the document embeddings to classify the text
        # this component uses all previous prepared components
        # document_embeddings: the embeddings to use
        # label_type: status --> sdg status
        # label_dictionary: the label dictionary to use
        # loss_weights: the weights for the loss function
        classifier = TextClassifier(document_embeddings, label_type="status", label_dictionary=label_dict_csv,
                                    loss_weights=weight_dict)

        # initialize the model trainer, which is an abstract API wrapper for training the classifier
        # classifier: the classifier to use
        # corpus: the corpus to use
        trainer = ModelTrainer(classifier, corpus)

        # call the fine_tune function of the model trainer
        # model_dir: the directory where the model will be saved
        # learning_rate: the learning rate for the fine-tuning (using literature recommended values)
        # mini_batch_size: the mini-batch size for the fine-tuning (bigger batch size --> CUDA out of memory error)
        # max_epochs: the maximum number of epochs for the fine-tuning (using literature recommended values)
        # train_with_dev: whether to use the dev set for training or not
        # monitor_train: whether to monitor the training or not
        # monitor_test: whether to monitor the testing or not

        # This function also is responsible for evaluating the model on the test set at the end of each fold
        trainer.fine_tune(model_dir + str(i + 1) + "/", learning_rate=3e-5, mini_batch_size=16, max_epochs=4,
                          train_with_dev=train_with_dev, monitor_train=True, monitor_test=True)


if __name__ == '__main__':
    main()
